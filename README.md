# Multi-modal IR search demo

This is a demo of several multi-modal IR search experiments. The goal of this exploration was to see if the retrieval of both articles and images stored in a single vector database using a single query is possible.

The demo is built using FastAPI, Streamlit, and Weaviate as the vector database.

Models used for this demo include [ALIGN](https://blog.research.google/2021/05/align-scaling-up-visual-and-vision.html) for feature extraction, [RAM and T2T](https://recognize-anything.github.io/) for caption generation, and a simple MLP to reduce the [heterogeniety gap](https://www.sciencedirect.com/science/article/pii/S0893608020304093#:~:text=The%20%E2%80%9Cheterogeneity%20gap%E2%80%9D%20means%20that,instances%20cannot%20be%20measured%20directly.) between text and image embeddings.

Several potential methods to achieve this have been explored:
- Finding similarity between base text and image embeddings generated by ALIGN.
- Using an MLP to map text embeddings to image embeddings.
- Using Weviate's hybrid search feature along with auto-generated captions.

The above methods are implemented in the demo. Results are quite unsatisfactory, but the demo is a good starting point for further exploration. Much better results are achieved by querying both images and articles separately using Weaviate's hybrid search feature.


# Setup
```
$ git clone
$ cd build
$ docker compose build
```

Before running the demo, make sure to include the weights for RAM, T2T and the MLP under /fastapi/models/weights. Then, run the following commands:


```
$ chmod +x start.sh
$ ./start.sh
```

View the demo at http://localhost:8501. FastAPI docs can be viewed at http://localhost:8000/docs.

# Todos:
- [ ] Model selection 
- [ ] Logging and error handling


# Architecture

